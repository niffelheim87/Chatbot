{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(url, visited):\n",
    "    \"\"\"Obtiene todos los enlaces de una página web y los filtra para evitar bucles.\"\"\"\n",
    "    links = set()\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for a_tag in soup.find_all('a', href=True):\n",
    "                full_url = urljoin(url, a_tag['href'])\n",
    "                if full_url.startswith(url) and full_url not in visited:\n",
    "                    links.add(full_url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error obteniendo enlaces de {url}: {e}\")\n",
    "    return links if links else set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Elimina caracteres especiales y basura del texto extraído.\"\"\"\n",
    "    # Mantener solo caracteres ASCII imprimibles y eliminar otros caracteres no deseados\n",
    "    cleaned_text = re.sub(r'[^\\x20-\\x7E\\n]', '', text)  # Mantiene solo caracteres ASCII imprimibles\n",
    "    # Filtra cadenas muy largas con caracteres no alfanuméricos\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s.,;!?@#$%^&*()_+-=<>/\\\\]{20,}', '', cleaned_text)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(url):\n",
    "    \"\"\"Extrae todo el texto de una página web y lo limpia de caracteres extraños.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        return clean_text(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extrayendo información de {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def categorize_text(text):\n",
    "    \"\"\"Clasifica el texto en categorías según palabras clave específicas.\"\"\"\n",
    "    categories = {\n",
    "        \"Contacto\": [],\n",
    "        \"Páginas Web\": [],\n",
    "        \"Ubicación\": [],\n",
    "        \"Educación\": []  # ✅ Guardará líneas completas solo si contienen una palabra clave exacta\n",
    "    }\n",
    "    \n",
    "    # Expresión regular para números de teléfono o fax (9 dígitos)\n",
    "    phone_fax_regex = r'\\b\\d{9}\\b'\n",
    "    \n",
    "    # Expresión regular para correos electrónicos con @gmail, @gva.es o @edu.gva\n",
    "    email_regex = r'\\b[A-Za-z0-9._%+-]+@(gmail\\.com|gva\\.es|edu\\.gva)\\b'\n",
    "    \n",
    "    # Expresión regular para detectar términos relacionados con ubicaciones\n",
    "    location_keywords = r'\\b(Adreça|Calle|Carrer|Avenida|Avda|Codi Postal|Ciutat|Ciudad|Adrea)\\b'\n",
    "    \n",
    "    # Lista de palabras clave exactas para información educativa\n",
    "    education_keywords = {\"ESO\", \"Educación Secundaria\", \"Bachillerato\", \"FP\", \n",
    "                          \"Formación Profesional\", \"Batxillerat\", \"Superior\", \"Artificial\"}\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        # Buscar números de teléfono o fax\n",
    "        if re.search(phone_fax_regex, line):\n",
    "            categories[\"Contacto\"].append(line)\n",
    "        # Buscar correos electrónicos con las extensiones permitidas\n",
    "        elif re.search(email_regex, line, re.IGNORECASE):\n",
    "            categories[\"Contacto\"].append(line)\n",
    "        # Buscar páginas web (enlaces)\n",
    "        elif re.match(r'^https?://', line):\n",
    "            categories[\"Páginas Web\"].append(line)\n",
    "        # Buscar información relacionada con la ubicación\n",
    "        elif re.search(location_keywords, line, re.IGNORECASE):\n",
    "            categories[\"Ubicación\"].append(line)\n",
    "        # Buscar información educativa y almacenar la línea completa solo si tiene una palabra clave exacta\n",
    "        else:\n",
    "            words = set(re.findall(r'\\b\\w+\\b', line))  # Extraer palabras sin signos de puntuación\n",
    "            if words & education_keywords:  # Si hay intersección con palabras clave, guardamos la línea\n",
    "                categories[\"Educación\"].append(line)\n",
    "    \n",
    "    return categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(base_url):\n",
    "    \"\"\"Extrae toda la información de una página web y sus enlaces y la organiza por categorías.\"\"\"\n",
    "    visited = set()\n",
    "    to_visit = {base_url}\n",
    "    categorized_data = {\n",
    "        \"Contacto\": set(),\n",
    "        \"Páginas Web\": set(),\n",
    "        \"Ubicación\": set(),\n",
    "        \"Educación\": set()  \n",
    "    }\n",
    "    \n",
    "    while to_visit:\n",
    "        url = to_visit.pop()\n",
    "        visited.add(url)\n",
    "        \n",
    "        text = extract_text(url)\n",
    "        if text:\n",
    "            page_data = categorize_text(text)\n",
    "            for category, content in page_data.items():\n",
    "                if category in categorized_data:  \n",
    "                    categorized_data[category].update(content)\n",
    "        \n",
    "        to_visit.update(get_all_links(url, visited) or set())\n",
    "    \n",
    "    intents = [\n",
    "        {\"tag\": category, \"patterns\": [f\"¿Qué información hay sobre {category.lower()}?\"], \"responses\": list(content)}\n",
    "        for category, content in categorized_data.items() if content\n",
    "    ]\n",
    "    \n",
    "    return {\"intents\": intents}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename=\"output.json\"):\n",
    "    \"\"\"Guarda la información en un archivo JSON.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en output.json\n"
     ]
    }
   ],
   "source": [
    "url = \"https://portal.edu.gva.es/iesserraperenxisa/\"\n",
    "scraped_data = scrape_website(url)\n",
    "save_to_json(scraped_data)\n",
    "print(\"Datos guardados en output.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
